# IDE Integration of LLMs

Large Language Models (LLMs) are increasingly becoming a powerful tool for enhancing developer productivity. In modern Integrated Development Environments (IDEs), they can function as intelligent code completion engines or as conversational assistants that help implement entire functions based on natural language input. This guide will walk you through the installation and usage of Tab-Code-Completion and Chat in Visual Studio Code.

## Installation Steps:

- Install/open Visual Studio Code.

- Access the Extension Marketplace: Click on Extensions (left toolbar).

- Search for "Continue": In the Marketplace search bar (located at the top left of the screen), type in "Continue". The first result you should see is "Continue – Codestral, Claude, and more".

- Click on "Continue – Codestral, Claude, and more" (first result at the top).

- In the main window, the overview of the extension appears. Click "Install" there.

- The extension will appear in the left toolbar (circle/square symbol). Click on it.

- A new column with an empty conversation history appears. Drag it from the top of the window to the right side of the screen. The column can be shown or hidden at any time using the modifier at the top of the menu bar.

- The Continue column has a gear icon at the bottom right for configuring the extension. Hover over it, and "Configure Continue" will appear. Click on it.

- The JSON configuration file will appear. In it, there are "models" and "tabAutocompleteModel" attributes that we will replace with our own configuration:

```
  "models": [
    {
      "model": "internlm2:7b-chat-v2.5-q8_0"",
      "provider": "ollama",
      "apiKey": "_",
      "apiBase": "http://localhost:11434/",
      "title": "internlm2:7b"
    },
    {
      "model": "qwen2.5-coder:7b-instruct-q8_0",
      "provider": "ollama",
      "apiKey": "_",
      "apiBase": "http://localhost:11434/",
      "title": "qwen2.5-coder:7b"
    }
  ],
  "tabAutocompleteModel": {
    "title": "AUTO",
    "provider": "ollama",
    "model": "starcoder2:3b",
    "apiKey": "",
    "apiBase": "http://localhost:11434/"
  },
```

If you have enabled different LLMs in your systen, take those LLMs and insert the model name
in the model attribute.

## Using the Continue Features
Once you have completed the installation and configuration, you can start taking advantage of the Continue features. 

- Autocomplete Code Suggestions: As you type in your program editor, autocomplete suggestions will appear in light gray. These suggestions are generated by the LLM based on your current input. To accept a suggestion, simply press the tab key.

- Refined Workflow with Comment-Driven Code Generation: To fully utilize the power of the LLM in code generation, it’s recommended to adopt a comment-first approach. Begin by writing a comment line that describes the functionality you want to implement. For instance, if you want to create a function that sorts an array, write a comment describing that. After pressing Enter, the Continue extension will automatically generate the corresponding code based on your comment.

- Request Full Functions via Chat: If you need more complex functionality, such as entire functions or methods, you can use the chat feature on the right-hand side. Ask the assistant to implement a function for you by describing what you need. The response will appear in a bordered box containing the generated code. To insert the code directly into your editor, click on the "Insert at cursor" button located in the top-right corner of the response box.

- Fix and Refactor Code: Continue also allows you to send code snippets directly to the assistant for review or refactoring. To do this, highlight a section of code in the editor and press Ctrl+L. The highlighted code will be transferred into the chat, where you can ask for fixes, improvements, or optimizations. The assistant will reply with a new version of the code, which you can insert back into your editor, replacing the old code.
